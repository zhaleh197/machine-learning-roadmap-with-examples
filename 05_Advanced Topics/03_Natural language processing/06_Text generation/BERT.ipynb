{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a popular model in the field of natural language processing (NLP) that has revolutionized various NLP tasks, such as text classification, question answering, named entity recognition, and sentiment analysis.\n",
    "\n",
    "BERT is a transformer-based neural network architecture introduced by Google in 2018. It is designed to capture the contextual relationships and representations of words in a sentence or text by leveraging bidirectional training. Unlike previous models that primarily used left-to-right or right-to-left context, BERT utilizes a bidirectional approach to consider both the left and right context simultaneously.\n",
    "\n",
    "The key idea behind BERT is the concept of masked language modeling and next sentence prediction. During the pre-training phase, BERT is trained on a large corpus of text by randomly masking certain words in the input and training the model to predict the masked words based on the context of the surrounding words. Additionally, BERT is also trained to predict whether two sentences appear consecutively in the original text or not.\n",
    "\n",
    "By pre-training BERT on large amounts of text data, it learns rich contextual representations of words and captures the nuances of language. These pre-trained representations can then be fine-tuned on specific downstream NLP tasks with smaller labeled datasets. This fine-tuning process allows BERT to adapt to specific tasks and achieve state-of-the-art performance with relatively fewer labeled examples.\n",
    "\n",
    "One of the key advantages of BERT is its ability to handle various NLP tasks in a unified manner. By using BERT as a pre-trained model and fine-tuning it on specific tasks, researchers and practitioners can achieve impressive results on different NLP benchmarks without the need for task-specific architectures.\n",
    "\n",
    "BERT has had a significant impact on the field of NLP and has inspired many subsequent transformer-based models. Its effectiveness in capturing contextual representations and its versatility across a range of NLP tasks have made BERT a widely adopted and influential model in the NLP community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "encoder_url='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "preprocess='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocessing_model = hub.KerasLayer(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_mask', 'input_word_ids', 'input_type_ids'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test=['hello world', ' I Love python programming']\n",
    "text_preprocessing=bert_preprocessing_model(text_test)\n",
    "text_preprocessing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing['input_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model=hub.KerasLayer(encoder_url)\n",
    "bert_results=bert_model(text_preprocessing)\n",
    "bert_results.key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results['pooled_output']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-based neural network\n",
    "\n",
    "A transformer-based neural network refers to a type of deep learning architecture that is based on the transformer model. The transformer model was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017 and has since become a fundamental building block in various natural language processing (NLP) tasks.\n",
    "\n",
    "Unlike traditional recurrent neural networks (RNNs) that process sequential data sequentially, the transformer model leverages a self-attention mechanism to capture dependencies and relationships across the entire input sequence simultaneously. This enables the model to effectively capture long-range dependencies and handle inputs of variable lengths.\n",
    "\n",
    "The key components of a transformer-based neural network are:\n",
    "\n",
    "1. Self-Attention Mechanism: This mechanism allows the model to weigh the importance of different words or tokens in the input sequence when encoding or decoding. It assigns attention scores to each word based on its relevance to other words in the sequence, allowing the model to focus on the most informative parts.\n",
    "\n",
    "2. Encoder-Decoder Architecture: Transformers often consist of an encoder and a decoder. The encoder processes the input sequence and encodes it into a set of hidden representations, while the decoder takes those representations and generates an output sequence.\n",
    "\n",
    "3. Positional Encoding: Since transformers do not have an inherent notion of word order, positional encoding is used to provide the model with positional information. Positional encodings are added to the input embeddings to convey the position of each word in the sequence.\n",
    "\n",
    "4. Multi-Head Attention: The self-attention mechanism is often applied with multiple attention heads, allowing the model to capture different types of dependencies at different positions in the sequence.\n",
    "\n",
    "Transformer-based neural networks have demonstrated remarkable performance on various NLP tasks, including machine translation, text summarization, sentiment analysis, and named entity recognition. They have become a widely used architecture due to their ability to handle long-range dependencies, their parallelizability, and their effectiveness in capturing contextual relationships in sequential data. Examples of popular transformer-based models include BERT, GPT, and Transformer-XL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
