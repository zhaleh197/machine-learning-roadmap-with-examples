{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple sample code for NLP using the Natural Language Toolkit (NLTK) library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " sample sentence . contains punctuation mark stopwords .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tech-8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tech-8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tech-8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Return the preprocessed text as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Sample text\n",
    "input_text = \"This is a sample sentence. It contains some punctuation marks and stopwords.\"\n",
    "\n",
    "# Preprocess the text\n",
    "processed_text = preprocess_text(input_text)\n",
    "\n",
    "print('output:\\n', processed_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called preprocess_text that performs basic text preprocessing tasks. It tokenizes the text into words, removes stopwords, and lemmatizes the words. The preprocessed text is then returned as a string.\n",
    "\n",
    "Make sure to have the NLTK library installed and download the required resources using nltk.download() function before running the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of other libraries such as SpaCy or TensorFlow depends on your specific tasks and requirements.\n",
    "\n",
    "SpaCy is a powerful NLP library that provides efficient tokenization, named entity recognition, part-of-speech tagging, and dependency parsing, among other features. It offers pre-trained models for various languages and allows you to perform advanced linguistic analysis on text data.\n",
    "\n",
    "TensorFlow, on the other hand, is a popular machine learning framework that provides a wide range of tools and functionalities for building and training deep learning models. It includes modules for text processing, sequence modeling, and language understanding, making it suitable for NLP tasks such as sentiment analysis, text classification, and machine translation.\n",
    "\n",
    "The choice of using SpaCy, TensorFlow, or any other NLP library depends on the specific tasks you want to accomplish and the features and capabilities you require. It's always a good idea to explore different libraries and choose the one that best suits your needs and offers the necessary functionality for your particular NLP project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
